# -*- coding: utf-8 -*-
"""Training_Setup.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13OhoQEAn-PjLU3hTketdHiemw6QmYXK7
"""

# =============================================================================
# 2) Enhanced Training Setup
# =============================================================================

# Import necessary classes from PyTorch's data utilities
from torch.utils.data import Dataset, DataLoader

class InstructionDataset(Dataset):
    """
    A custom PyTorch Dataset to handle instruction-formatted text data.
    This class is responsible for taking a list of raw text strings and converting
    them into tokenized tensors that can be fed into the model.
    """
    def __init__(self, texts, tokenizer, max_length=512):
        """
        Initializes the dataset.
        Args:
            texts (list[str]): A list of raw text strings for training.
            tokenizer: The Hugging Face tokenizer to be used.
            max_length (int): The maximum sequence length for padding and truncation.
        """
        self.tokenizer = tokenizer
        self.texts = texts
        self.max_length = max_length

    def __len__(self):
        """Returns the total number of samples in the dataset."""
        return len(self.texts)

    def __getitem__(self, idx):
        """
        Retrieves and processes a single sample from the dataset.
        This method is called by the DataLoader for each item in a batch.
        """
        # Get the raw text at the specified index.
        text = self.texts[idx]

        # Tokenize the text using the provided tokenizer.
        # - max_length: Ensures all sequences are the same length.
        # - padding="max_length": Adds padding tokens to shorter sequences.
        # - truncation=True: Cuts off sequences longer than max_length.
        # - return_tensors="pt": Returns the output as PyTorch tensors.
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )

        # The tokenizer returns tensors with a batch dimension of 1, so we .squeeze()
        # to remove it, as the DataLoader will re-add the batch dimension later.
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze()
        }

def replace_with_svd_lora(module, alpha, lora_rank, dtype):
    """
    Recursively traverses the model's modules and replaces target nn.Linear layers
    with our custom SVD_LoRA_Linear layer. This function performs the "model surgery".
    """
    # Define the names of the linear layers we want to target for replacement.
    TARGETS = {'q_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'gate_proj'}

    # Keep track of how many layers we successfully replace.
    replaced_count = 0

    # Iterate over a static copy of the children, as we are modifying the module in-place.
    for name, child in list(module.named_children()):
        # If the child is a ModuleList (like the list of transformer blocks),
        # we need to recurse into each of its sub-modules.
        if isinstance(child, nn.ModuleList):
            for sub_child in child:
                replaced_count += replace_with_svd_lora(sub_child, alpha, lora_rank, dtype)

        # If the child is a Linear layer AND its name is in our target list...
        elif isinstance(child, nn.Linear) and name in TARGETS:
            # Calculate the SVD compression rank based on the alpha ratio.
            min_dim = min(child.in_features, child.out_features)
            compression_rank = max(1, int(alpha * min_dim))

            # Replace the original layer with our new hybrid layer.
            setattr(module, name, SVD_LoRA_Linear(child, compression_rank, lora_rank, dtype))
            replaced_count += 1

        # For any other type of module that might contain linear layers, recurse into it.
        else:
            replaced_count += replace_with_svd_lora(child, alpha, lora_rank, dtype)

    return replaced_count

def setup_model_for_lora_training(model):
    """
    Prepares the hybrid model for Parameter-Efficient Fine-Tuning (PEFT).
    This function freezes the entire model and then un-freezes only the
    newly added LoRA adapter parameters.
    """
    # Iterate through all named parameters in the model.
    for name, param in model.named_parameters():
        # The `requires_grad` attribute controls whether a parameter will be updated
        # during training.
        if "lora_" in name:
            # If the parameter is part of our LoRA adapter, make it trainable.
            param.requires_grad = True
        else:
            # Otherwise, freeze it. This includes the SVD base and all other original weights.
            param.requires_grad = False

    # Calculate and print a report on the parameter efficiency.
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nTrainable params: {trainable_params/1e6:.2f}M ({100 * trainable_params / total_params:.4f}% of total)")

    return model