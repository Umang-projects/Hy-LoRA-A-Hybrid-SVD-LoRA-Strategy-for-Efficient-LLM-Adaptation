# -*- coding: utf-8 -*-
"""Perplexity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13OhoQEAn-PjLU3hTketdHiemw6QmYXK7
"""

# =============================================================================
# 3) Improved Perplexity Calculation with Batching
# =============================================================================

# Import necessary classes from PyTorch's data utilities
from torch.utils.data import DataLoader
from tqdm import tqdm
import math

def calculate_perplexity(model, tokenizer, texts, device="cuda", max_length=256, batch_size=8):
    """
    Calculates the perplexity of a model on a given set of texts using an
    efficient, batched approach.

    This is more robust and accurate than a simple streaming approach because:
    1. It processes data in batches, which is much more efficient for GPUs.
    2. It calculates a true token-level perplexity by weighting the loss of each
       batch by the number of non-padding tokens.

    Args:
        model: The Hugging Face transformer model to evaluate.
        tokenizer: The tokenizer corresponding to the model.
        texts (list[str]): A list of raw text strings for evaluation.
        device (str): The device to run the evaluation on (e.g., "cuda").
        max_length (int): The maximum sequence length for tokenization.
        batch_size (int): The number of texts to process in each batch.

    Returns:
        float: The calculated perplexity score.
    """
    # Set the model to evaluation mode. This disables layers like Dropout.
    model.eval().to(device)

    # Initialize variables to accumulate the total loss and the total number of tokens.
    total_loss = 0.0
    total_tokens = 0

    # --- Data Preparation ---
    # Create a custom Dataset object from our raw texts.
    dataset = InstructionDataset(texts, tokenizer, max_length)
    # Create a DataLoader to efficiently iterate over the dataset in batches.
    # shuffle=False is important for evaluation to ensure consistent results.
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

    # --- Evaluation Loop ---
    # `torch.no_grad()`: Disables gradient calculation, saving memory and speeding up inference.
    # `torch.cuda.amp.autocast()`: Enables Automatic Mixed Precision. It runs operations
    # in bfloat16 for performance but keeps sensitive operations in float32 for stability.
    with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):
        # Iterate over each batch provided by the DataLoader.
        for batch in tqdm(dataloader, desc="Calculating Perplexity"):
            # Move the batch of data to the specified GPU/CPU device.
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            # --- Label Preparation for Loss Calculation ---
            # For causal language models, the labels are the same as the input_ids.
            labels = input_ids.clone()

            # The Hugging Face CrossEntropyLoss function is hard-coded to ignore a label of -100.
            # We replace the token ID of padding tokens with -100 so that the loss is not
            # calculated for these tokens, giving a more accurate perplexity score.
            labels[labels == tokenizer.pad_token_id] = -100

            # --- Forward Pass ---
            # When `labels` are provided, the model automatically calculates the loss.
            outputs = model(input_ids=input_ids,
                            attention_mask=attention_mask,
                            labels=labels)

            loss = outputs.loss

            # A robustness check to skip any batches that result in a NaN loss.
            if torch.isnan(loss):
                continue

            # --- Loss Accumulation (Token-Weighted) ---
            # The `attention_mask` is 1 for real tokens and 0 for padding tokens.
            # Summing the mask gives the exact number of real tokens in the batch.
            num_tokens = attention_mask.sum().item()

            # The `loss` from the model is the *average* loss per token. To get the
            # total loss for the batch, we must "un-average" it by multiplying by the number of tokens.
            total_loss += loss.item() * num_tokens
            total_tokens += num_tokens

    # --- Final Calculation ---
    # A robustness check in case the dataset was empty or all batches failed.
    if total_tokens == 0:
        return float('inf')

    # Calculate the true average loss across all non-padding tokens in the entire dataset.
    avg_loss = total_loss / total_tokens

    # Perplexity is defined as the exponential of the cross-entropy loss.
    return math.exp(avg_loss)